{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'G:/github/lesson9/data/sample_data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-ee7fa35f1c10>:54: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Let's build a small network with two convolutional layers, followed by one fully connected layer.\n",
    "Convolutional networks are more expensive computationally, \n",
    "so we'll limit its depth and number of fully connected nodes.\n",
    "\"\"\"\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  \"\"\"Input data.\n",
    "   input : 输入的要做卷积的图片，要求为一个张量，shape为 [ batch, in_height, in_weight, in_channel ]，\n",
    "   其中batch为图片的数量，in_height 为图片高度，in_weight 为图片宽度，in_channel 为图片的通道数，灰度\n",
    "   图该值为1，彩色图为3。（也可以用其它值，但是具体含义不是很理解）\n",
    "  \"\"\"\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.499545\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.856652\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 29.2%\n",
      "Minibatch loss at step 100: 0.522452\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 67.7%\n",
      "Minibatch loss at step 150: 0.896857\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 73.9%\n",
      "Minibatch loss at step 200: 1.091586\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 74.5%\n",
      "Minibatch loss at step 250: 1.154553\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 300: 0.391969\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 350: 1.111692\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 400: 0.871404\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 450: 0.419640\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 500: 0.916143\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 550: 0.355345\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 600: 0.641343\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 650: 0.314513\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 700: 0.617087\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 750: 0.293878\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 800: 0.663021\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 850: 0.510488\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 900: 0.660551\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 950: 0.388013\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1000: 0.177706\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 90.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (nn.max_pool()) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, train=False):\n",
    "    \"\"\"    \n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "   \"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        layer1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, layer1_biases))\n",
    "    # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        layer2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, layer2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "      hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 4.337938\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.596911\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 56.2%\n",
      "Minibatch loss at step 100: 0.608157\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 150: 0.690519\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 200: 0.770048\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.7%\n",
      "Minibatch loss at step 250: 0.849430\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 300: 0.406745\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 350: 1.038926\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 79.0%\n",
      "Minibatch loss at step 400: 0.939514\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 450: 0.413870\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 500: 0.678123\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 550: 0.253983\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 600: 0.626769\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 650: 0.356043\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 700: 0.444330\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 750: 0.340204\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 800: 0.479360\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 850: 0.474772\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 900: 0.591595\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 950: 0.412165\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 1000: 0.103922\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.4%\n",
      "Test accuracy: 91.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import sys\n",
    "import os \n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "logs_train_dir = 'G:/lessons/lesson12/model' \n",
    "def weight_variable(shape):   \n",
    "    # 产生正态分布，标准差为0.1    \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)    \n",
    "    return tf.Variable(initial)  \n",
    "def bias_variable(shape):    # 创建一个结构为shape矩阵也可以说是数组shape声明其行列，初始化所有值为0.1    \n",
    "    initial = tf.constant(0.1, shape=shape)    \n",
    "    return tf.Variable(initial)  \n",
    "def conv2d(x,W):    \n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')  \n",
    "def max_pool_2x2(x):    \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  \n",
    "def inference(images,drop = False):    \n",
    "    # 第1层卷积    \n",
    "    with tf.variable_scope('conv1'):        \n",
    "        W_conv1 = tf.Variable(weight_variable([5, 5, 1, 6]), name=\"weight\")        \n",
    "        b_conv1 = tf.Variable(bias_variable([6]), name=\"bias\")        \n",
    "        h_conv1 = tf.nn.relu(conv2d(images, W_conv1) + b_conv1)    \n",
    "        # print(np.shape(h_conv1))   \n",
    "    # 第2层池化    \n",
    "    with tf.variable_scope('pool2'):        \n",
    "        h_pool2 = max_pool_2x2(h_conv1)    \n",
    "        # print(np.shape(h_pool2))     \n",
    "    # 第3层卷积    \n",
    "    with tf.variable_scope('conv3'):        \n",
    "         W_conv3 = tf.Variable(weight_variable([5, 5, 6, 16]), name=\"weight\")        \n",
    "         b_conv3 = tf.Variable(bias_variable([16]), name=\"bias\")        \n",
    "         h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)    \n",
    "         # print(np.shape(h_conv3))    \n",
    "    # 第4层池化    \n",
    "    with tf.variable_scope('pool4'):        \n",
    "          h_pool4 = max_pool_2x2(h_conv3)    \n",
    "          # print(np.shape(h_pool4))    \n",
    "        \n",
    "    h_pool4_flat = tf.reshape(h_pool4, [-1, 7 * 7 * 16])  \n",
    "    \n",
    "    # 5\\6\\7全连接层    \n",
    "    with tf.variable_scope('fc5'):        \n",
    "         W_fc5 = weight_variable([7 * 7 * 16, 120])        \n",
    "         b_fc5 = bias_variable([120])       \n",
    "         h_fc5 = tf.nn.relu(tf.matmul(h_pool4_flat, W_fc5) + b_fc5) \n",
    "            \n",
    "    with tf.variable_scope('fc6'):        \n",
    "         W_fc6 = weight_variable([120, 84])        \n",
    "         b_fc6 = bias_variable([84])        \n",
    "         h_fc6 = tf.nn.relu(tf.matmul(h_fc5, W_fc6) + b_fc6)  \n",
    "         if drop:\n",
    "             h_fc6  = tf.nn.dropout(h_fc6 , 0.5)   \n",
    "            \n",
    "    with tf.variable_scope('out'):        \n",
    "         W_out = weight_variable([84, 10])        \n",
    "         b_out = bias_variable([10])        \n",
    "         h_out = tf.nn.softmax(tf.matmul(h_fc6, W_out) + b_out) \n",
    "     \n",
    "    return h_out \n",
    "def train_fun(drop = False):\n",
    "    x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1]) \n",
    "    y_conv = inference(x_image,drop) \n",
    "    # 定义损失函数和学习步骤\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))   \n",
    "    # 交叉熵\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)   \n",
    "    # 用Adam优化器 \n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter(logs_train_dir, sess.graph) \n",
    "    for i in range(2000):    \n",
    "        batch = mnist.train.next_batch(50)   \n",
    "        if i % 100 == 0:        \n",
    "            train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})        \n",
    "            print(\"step %d, training accuracy %g\" % (i, train_accuracy))        \n",
    "            # summary_str = sess.run(summary_op)       \n",
    "            # train_writer.add_summary(summary_str, i)    \n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    checkpoint_path = os.path.join(logs_train_dir, 'thing.ckpt')\n",
    "    saver.save(sess, checkpoint_path)\n",
    "    print(\"test accuracy %g\" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n",
      "step 100, training accuracy 0.98\n",
      "step 200, training accuracy 0.94\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 0.98\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.98\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 0.96\n",
      "step 1000, training accuracy 1\n",
      "step 1100, training accuracy 0.98\n",
      "step 1200, training accuracy 0.98\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 1\n",
      "step 1500, training accuracy 0.96\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 1\n",
      "step 1800, training accuracy 0.98\n",
      "step 1900, training accuracy 1\n",
      "test accuracy 0.9862\n"
     ]
    }
   ],
   "source": [
    "train_fun(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.06\n",
      "step 100, training accuracy 0.84\n",
      "step 200, training accuracy 0.82\n",
      "step 300, training accuracy 0.88\n",
      "step 400, training accuracy 0.94\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.96\n",
      "step 700, training accuracy 0.94\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.98\n",
      "step 1100, training accuracy 0.94\n",
      "step 1200, training accuracy 0.96\n",
      "step 1300, training accuracy 1\n",
      "step 1400, training accuracy 0.96\n",
      "step 1500, training accuracy 0.96\n",
      "step 1600, training accuracy 1\n",
      "step 1700, training accuracy 0.96\n",
      "step 1800, training accuracy 0.96\n",
      "step 1900, training accuracy 0.94\n",
      "test accuracy 0.978\n"
     ]
    }
   ],
   "source": [
    "train_fun(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
