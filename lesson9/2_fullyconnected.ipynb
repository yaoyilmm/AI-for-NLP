{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pickle_file = 'G:/github/lesson9/data/sample_data/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "  image_size = 28\n",
    "  num_labels = 10\n",
    "\n",
    "\n",
    "  def reformat(dataset, labels):\n",
    "      dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "      # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "      labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "      return dataset, labels\n",
    "\n",
    "\n",
    "  train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "  valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "  test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 我们首先要使用简单的梯度下降来训练多项Logistic回归。\n",
    "TensorFlow的工作方式如下：\n",
    "首先，您描述要执行的计算：输入，变量和操作的外观。 这些作为计算图上的节点创建。 此描述全部包含在以下块中：\n",
    "与graph.as_default（）：\n",
    "    ...\n",
    "然后，您可以通过调用session.run（）来多次在此图上运行操作，从而提供输出以从返回的图形中获取。 此运行时操作全部包含在下面的块中：\n",
    "使用tf.Session（graph = graph）作为会话：\n",
    "    ...\n",
    "让我们将所有数据加载到TensorFlow中，并构建与我们的训练相对应的计算图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 16.816549\n",
      "Training accuracy: 14.2%\n",
      "Validation accuracy: 16.7%\n",
      "Loss at step 100: 2.285333\n",
      "Training accuracy: 72.7%\n",
      "Validation accuracy: 70.0%\n",
      "Loss at step 200: 1.827454\n",
      "Training accuracy: 75.4%\n",
      "Validation accuracy: 72.5%\n",
      "Loss at step 300: 1.581783\n",
      "Training accuracy: 76.8%\n",
      "Validation accuracy: 73.2%\n",
      "Loss at step 400: 1.416703\n",
      "Training accuracy: 77.6%\n",
      "Validation accuracy: 74.0%\n",
      "Loss at step 500: 1.294186\n",
      "Training accuracy: 78.4%\n",
      "Validation accuracy: 74.2%\n",
      "Loss at step 600: 1.197739\n",
      "Training accuracy: 78.8%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 700: 1.119179\n",
      "Training accuracy: 79.4%\n",
      "Validation accuracy: 74.9%\n",
      "Loss at step 800: 1.053909\n",
      "Training accuracy: 79.7%\n",
      "Validation accuracy: 75.1%\n",
      "Test accuracy: 83.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "def accuracy(predictions, labels):\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "                / predictions.shape[0])\n",
    "\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "        # This is a one-time operation which ensures the parameters get initialized as\n",
    "        # we described in the graph: random weights for the matrix, zeros for the\n",
    "        # biases.\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        for step in range(num_steps):\n",
    "            # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "            # and get the loss value and the training predictions returned as numpy\n",
    "            # arrays.\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "            if (step % 100 == 0):\n",
    "                print('Loss at step %d: %f' % (step, l))\n",
    "                print('Training accuracy: %.1f%%' % accuracy(\n",
    "                    predictions, train_labels[:train_subset, :]))\n",
    "                # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "                # just to get that one numpy array. Note that it recomputes all its graph\n",
    "                # dependencies.\n",
    "                print('Validation accuracy: %.1f%%' % accuracy(\n",
    "                    valid_prediction.eval(), valid_labels))\n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 现在让我们转而使用随机梯度下降训练，速度要快得多。\n",
    "该图将是类似的，除了不是将所有训练数据保存到一个常量节点，我们创建一个占位符节点，它将在每次调用session.run（）时提供实际数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 16.479559\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 16.2%\n",
      "Minibatch loss at step 500: 1.830459\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 1000: 1.153251\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 1500: 0.959731\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 2000: 1.062314\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 2500: 1.554601\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 3000: 0.589183\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.5%\n",
      "Test accuracy: 86.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题\n",
    "将具有SGD的逻辑回归示例转换为具有整流线性单位nn.relu（）和1024个隐藏节点的1-隐藏层神经网络。 该模型应该可以提高验证/测试的准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#封装的层函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layers(input_data, weight, baies):\n",
    "    with tf.name_scope('layer_1'): \n",
    "        logits_1 = tf.matmul(input_data, weight['w1']) + baies['b1']\n",
    "    with tf.name_scope('relu'):\n",
    "        hidden_layer = tf.nn.relu(logits_1, name='hidden_layer')\n",
    "    with tf.name_scope('layer_2'):\n",
    "        logits_2 = tf.matmul(hidden_layer, weight['w2']) + baies['b2']\n",
    "    return logits_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 406.098145\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 37.0%\n",
      "Minibatch loss at step 500: 16.072044\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 1000: 5.554845\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.5%\n",
      "Minibatch loss at step 1500: 7.267047\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 2000: 4.214048\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2500: 7.638156\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 3000: 1.271598\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "hidden_units = 1024\n",
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('weight'):\n",
    "            weight = {\n",
    "                    \"w1\": tf.Variable(tf.truncated_normal([image_size * image_size, hidden_units])),\n",
    "                    \"w2\": tf.Variable(tf.truncated_normal([hidden_units, num_labels]))\n",
    "            }\n",
    "    with tf.name_scope('baies'):\n",
    "            baies = {\"b1\": tf.Variable(tf.zeros([hidden_units])), \"b2\": tf.Variable(tf.zeros([num_labels]))}\n",
    "            \n",
    "    #封装的层函数\n",
    "    def multi_layers(input_data, weight, baies):\n",
    "        with tf.name_scope('layer_1'): \n",
    "            logits_1 = tf.matmul(input_data, weight['w1']) + baies['b1']\n",
    "        with tf.name_scope('relu'):\n",
    "            hidden_layer = tf.nn.relu(logits_1, name='hidden_layer')\n",
    "        with tf.name_scope('layer_2'):\n",
    "            logits_2 = tf.matmul(hidden_layer, weight['w2']) + baies['b2']\n",
    "        return logits_2\n",
    "    \n",
    "    with tf.name_scope('input_data'):\n",
    "        with tf.name_scope('train_data'):\n",
    "             tf_train_data = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    with tf.name_scope('train_labels'):\n",
    "         tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    with tf.name_scope('valid_data'): \n",
    "         tf_valid_data = tf.constant(valid_dataset)\n",
    "    with tf.name_scope('test_data'):\n",
    "          tf_test_data = tf.constant(test_dataset)\n",
    "            \n",
    "    with tf.name_scope('loss'):\n",
    "        predict = multi_layers(tf_train_data, weight, baies)\n",
    "        loss = tf.reduce_mean( \n",
    "               tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=predict, name='loss'))\n",
    "    with tf.name_scope('optimizer'):\n",
    "         optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    with tf.name_scope('train_prediction'):        \n",
    "         train_prediction = tf.nn.softmax(predict) \n",
    "    with tf.name_scope('valid_prediction'):\n",
    "        valid_predict = multi_layers(valid_dataset, weight, baies) \n",
    "        valid_prediction = tf.nn.softmax(valid_predict)\n",
    "    with tf.name_scope('test_prediction'): \n",
    "        test_predict = multi_layers(test_dataset, weight, baies)\n",
    "        test_prediction = tf.nn.softmax(test_predict)\n",
    "    num_steps = 3001\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        writer = tf.summary.FileWriter(\"logss/\", session.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            #  Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_data: batch_data, tf_train_labels: batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "                   [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if step % 500 == 0:\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                       valid_prediction.eval(), valid_labels))\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
