{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  jieba\n",
    "import  re\n",
    "import  string\n",
    "stop_words_file = \"G:/github/lesson7-8/stop_words/stop_words.utf8\"\n",
    "def cut(string):\n",
    "    token =  list(jieba.cut(string))\n",
    "    token = [word.strip() for word in token]\n",
    "    return token\n",
    "def get_stop_words_list():\n",
    "    with open(stop_words_file,encoding=\"utf-8\") as f:\n",
    "       stop_words_list = f.readlines()\n",
    "    return stop_words_list\n",
    "#去除特殊符号\n",
    "def remove_special_characters(text):\n",
    "    tokens = cut(text)\n",
    "    pattern = re.compile(\"[{0}\\d+]\".format(re.escape(string.punctuation)) )#print(string.punctuation) = !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    filtered_tokens =  filter(None,[pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "#去停用词\n",
    "def remove_stopwords(text):\n",
    "    tokens = cut(text)\n",
    "    stop_words_list = get_stop_words_list()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words_list] #这地方根本没有去停用词，如果去除了停用词，最后的评估模型的得分居然变低了。。。\n",
    "    filtered_text = ''.join(filtered_tokens) #去掉空格\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    #print(corpus)\n",
    "    for text in corpus:\n",
    "        text = remove_special_characters(text)\n",
    "        text = remove_stopwords(text)\n",
    "       # normalized_corpus.append(text)\n",
    "        if tokenize:    #tokenize=True时执行\n",
    "            text = ' '.join(cut(text))\n",
    "        normalized_corpus.append(text)\n",
    "        #print(text)\n",
    "    #仅仅返回的是去掉特殊符号的文本\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女性 len = 260\n",
      "体育 len = 260\n",
      "文学 len = 260\n",
      "校园 len = 260\n",
      "shap2, (1040, 11983)\n",
      "['ac', 'act', 'arsenal', 'ata', 'a股', 'bb', 'bbb', 'boss', 'by', 'ca']\n",
      "clusters= [0 2 2 ... 0 1 1]\n",
      "news_data['cluster']= [0 2 2 ... 0 1 1]\n",
      "dict_items([(0, 202), (2, 114), (1, 686), (3, 38)])\n",
      "news_data=\n",
      "Cluster 0 details:\n",
      "--------------------\n",
      "Key features: ['标签', '转发', '微博', '来自', '评论', '新浪', '上海', '今天', '周刊', '乔布斯', '星座', '体育', '出版', '时光', '第期', '中国', '英超', '直播', '皮皮', '狮子座']\n",
      "Cluster 1 details:\n",
      "--------------------\n",
      "Key features: ['学校', '我们', '中国', '足球', '学生', '一个', '标签', '体育', '环保', '分享', '大学生', '可以', '校车', '腾讯', '女人', '皮肤', '自己', '校园', '学院', '没有']\n",
      "Cluster 2 details:\n",
      "--------------------\n",
      "Key features: ['原文', '转发', '杂志', '评论', '微博', '面膜', '时尚', '意林', '推荐', '中国', '我们', '皮肤', '感恩', '时间', '好书', '女性', '可以', '支持', '粉丝', '新浪']\n",
      "Cluster 3 details:\n",
      "--------------------\n",
      "Key features: ['nba', '排名', '腾讯', '直播', '中国', '巨星', '足球', '指数', '人均', '国际足球', '大帅府', '视频', '东北', '热烈欢迎', 'helloqqusersiveopenedthisaccounttobeclosertomychinesefansandhappytoshareexclusivecontentswithyoustaytuned', 'lotozfc', '世界足球', '高于', '先生', '入驻']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jieba\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from  sklearn.feature_extraction.text import  CountVectorizer\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def cut_words(file_path):\n",
    "    text_with_spaces = ''\n",
    "    text=open(file_path, 'r', encoding='gb18030').read()\n",
    "    return text\n",
    "\n",
    "def loadfile(file_dir, label):\n",
    "    file_list = os.listdir(path + file_dir)\n",
    "    words_list = []\n",
    "    labels_list = []\n",
    "    for file in file_list:\n",
    "        file_path = path + file_dir + '/' + file\n",
    "        words_list.append(cut_words(file_path))\n",
    "        labels_list.append(label)\n",
    "    words_list = normalize_corpus(words_list, True)\n",
    "    return words_list, labels_list\n",
    "def build_feature_matrix(documents, feature_type='frequency',\n",
    "                         ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n",
    "    feature_type = feature_type.lower().strip()\n",
    "\n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True,\n",
    "                                     max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range)\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                 norm='l2',max_df =max_df,\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,ngram_range=ngram_range)\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "\n",
    "    return vectorizer, feature_matrix\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "def k_means(feature_matrix, num_clusters=4):\n",
    "    km = KMeans(n_clusters=num_clusters,\n",
    "                max_iter=10000)\n",
    "    km.fit(feature_matrix)\n",
    "    clusters = km.labels_\n",
    "    print(\"clusters=\",clusters)\n",
    "    return km, clusters\n",
    "\n",
    "def get_cluster_data(clustering_obj, book_data,\n",
    "                     feature_names, num_clusters,\n",
    "                     topn_features=20):\n",
    "    cluster_details = {}\n",
    "    # 获取cluster的center\n",
    "    ordered_centroids = clustering_obj.cluster_centers_.argsort()[:, ::-1]\n",
    "    # 获取每个cluster的关键特征\n",
    "    # 获取每个cluster的新闻\n",
    "    for cluster_num in range(num_clusters):\n",
    "        cluster_details[cluster_num] = {}\n",
    "        cluster_details[cluster_num]['cluster_num'] = cluster_num\n",
    "        key_features = [feature_names[index]\n",
    "                        for index\n",
    "                        in ordered_centroids[cluster_num, :topn_features]]\n",
    "        cluster_details[cluster_num]['key_features'] = key_features\n",
    "    return cluster_details\n",
    "\n",
    "\n",
    "def print_cluster_data(cluster_data):\n",
    "    # print cluster details\n",
    "    for cluster_num, cluster_details in cluster_data.items():\n",
    "        print('Cluster {} details:'.format(cluster_num))\n",
    "        print('-' * 20)\n",
    "        print('Key features:', cluster_details['key_features'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 训练数据\n",
    "    path = \"G:/github/lesson7-8/data/\"\n",
    "    train_words_list1, train_labels1 = loadfile('train/女性', '0')\n",
    "\n",
    "    train_words_list2, train_labels2 = loadfile('train/体育', '1')\n",
    "\n",
    "    train_words_list3, train_labels3 = loadfile('train/文学', '2')\n",
    "\n",
    "    train_words_list4, train_labels4 = loadfile('train/校园', '3')\n",
    "    len_value = 260\n",
    "    train_words_list = train_words_list1[:len_value] + train_words_list2[:len_value] + train_words_list3[:len_value] + train_words_list4[:len_value]\n",
    "\n",
    "    train_labels = train_labels1 + train_labels2 + train_labels3 + train_labels4\n",
    "\n",
    "    print(\"女性 len =\" ,len(train_words_list1[:len_value]))\n",
    "    print(\"体育 len =\", len(train_words_list2[:len_value]))\n",
    "    print(\"文学 len =\", len(train_words_list3[:len_value]))\n",
    "    print(\"校园 len =\", len(train_words_list4[:len_value]))\n",
    "\n",
    "    news_data = {}\n",
    "    news_data[\"content\"] = train_words_list\n",
    "    news_data[\"type\"] = train_labels\n",
    "    stop_words = open(stop_words_file, 'r', encoding='utf-8').read()\n",
    "    stop_words = stop_words.encode('utf-8').decode('utf-8-sig')  # 列表头部\\ufeff处理\n",
    "    stop_words = stop_words.split('\\n')  # 根据分隔符分隔\n",
    "    # 提取 tf-idf 特征\n",
    "    vectorizer, train_features = build_feature_matrix(train_words_list,\n",
    "                                                      feature_type='tfidf',\n",
    "                                                      min_df=0.1, max_df=0.60,\n",
    "                                                      ngram_range=(1, 1))\n",
    "    # 查看特征数量\n",
    "    print(\"shap2,\",train_features.shape)\n",
    "    # 获取特征名字\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    # 打印某些特征\n",
    "    print(feature_names[:10])\n",
    "    num_clusters = 4\n",
    "    km_obj, clusters = k_means(feature_matrix=train_features,\n",
    "                               num_clusters=num_clusters)\n",
    "    news_data['Cluster'] = clusters\n",
    "    print(\"news_data['cluster']=\", news_data['Cluster'])\n",
    "    from collections import Counter\n",
    "\n",
    "    # 获取每个cluster的数量\n",
    "    c = Counter(clusters)\n",
    "    print(c.items())\n",
    "    print(\"news_data=\")\n",
    "    cluster_data = get_cluster_data(clustering_obj=km_obj,\n",
    "                                    book_data=news_data,\n",
    "                                    feature_names=feature_names,\n",
    "                                    num_clusters=num_clusters,\n",
    "                                    topn_features=20)\n",
    "\n",
    "    print_cluster_data(cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#样本数量必须平衡，聚类才效果才稍微明显一些，感觉k-means聚类只是大致的分类，效果不是很好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
